---
title: "Naive Bayes from Scratch"
author: "Kevin Patyk"
date: "1/31/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.align = 'center')
```

# Introduction 

Naive Bayes is a rather simple method for constructing probabilistic classifiers. Naive Bayes, also called simple Bayes and independent Bayes, uses the assumption that a value of a particular feature is independent of the value of any other feature in the dataset, given the class variable. For example, a fruit may be considered  an apple if it is red, round, and about 8 cm in diameter. A naive Bayes classifier considers each of these features to contribute independently to the probability that this fruit is an apple, regardless of any possible correlations between the color, roundness, and diameter features.

Despite the simplicity of the algorithm and its oversimplified assumption, naive Bayes tends to perform well in real-world, complex situations. Naive Bayes has, even in some cases, outperformed more sophisticated algorithms, such as boosted trees or decision trees. Additionally, the naive Bayes classifier has the advantages of being easy to build and only require a small number of training data to estimate the parameters necessary for classification.

Since naive Bayes is a probabilistic algorithm, it takes advantage of probability theory. Probabilistic means that the algorithm calculates the probability of each class for a given observation, and then outputs the class with the highest one. The way they get these probabilities is by using Bayesâ€™ theorem. More specifically, Bayes' theorem is a way of obtaining conditional probability. Conditional probability is a measure of the probability of an event occurring, given that another event (by assumption, presumption, assertion or evidence) has already occurred. Bayes' theorem provides a way to revise existing predictions or theories (updating probabilities) given new or additional evidence.

The formula for Bayes' theorem is:

$$ P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$
Where: 

* $P(A|B)$ is a conditional probability. This is the probability of event $A$ occurring given that $B$ has occurred. It is also called the posterior probability of $A$ given $B$. Posterior probability is the revised probability of an event occurring after taking into consideration new information. Posterior probability is calculated by updating the prior probability by using Bayes' theorem. This is the probability value that has been revised by using new information that is later obtained from a subsequent event. In other words, this is the updated belief about the hypothesis as new evidence becomes available.

* $P(B|A)$ is also a conditional probability. This is the probability of event $B$ occurring given that $A$ has occurred. This can also be interpreted as the likelihood of $A$ given a fixed $B$. 

* $P(A)$ and $P(B)$ are the probabilities of observing $A$ and $B$ respectively without any given conditions. $P(A)$ can be called the prior probability. This is the initial probability about an event before any information is available about the event. In other words, this is the initial belief about a particular hypothesis before any evidence is available about the hypothesis.

Below is a helpful image to understand Bayes' theorem:

<p align="center">
![](D:\UU\Independent Projects\Naive Bayes from Scratch\Naive Bayes\Images\Bayes_Theorem.PNG){height=450px}
</p>

Since we are not working directly with probabilities and we want to predict $y$ from $x$, the Bayes' theorem can be written as:

$$ P(y|X) = \frac{P(X|y)P(y)}{P(X)}$$

where $X$ is a feature vector $X = (x_1, x_2, x_3, ..., x_n)$.

Assuming that all the features are mutually independent, we can write the Bayes' theorem as: 

$$ P(y|X) = \frac{P(x_1|y) \cdot P(x_1|y) \cdot ... \cdot P(x_n|y) \cdot P(y)}{P(X)}$$

Given the posterior probability $P(y|X)$, we want to select the class with the highest probability. So, we choose:

$$y = \text{ argmax}_y P(y|X) = \text{ argmax}_y \frac{P(x_1|y) \cdot P(x_1|y) \cdot ... \cdot P(x_n|y) \cdot P(y)}{P(X)}$$

And, since we are only interested in $y$, we do not need to denominator $P(X)$, only leaving the numerator, which are the class conditional probabilities multiplied by the prior probability. 

$$y = \text{ argmax}_y P(x_1|y) \cdot P(x_1|y) \cdot ... \cdot P(x_n|y) \cdot P(y)$$

Finally, we are using the natural logarithm. This is because all of the probabilities are between 0 and 1. Multiplying many probabilities together can result in a really small number and we might run into underflow problems. Underflow is the generation of a number that is too small to be represented in the device meant to store it, which means the computer will just set it to 0 and cause complications with our analysis.

Thus, the formula now is:

$$y = \text{ argmax}_y log(P(x_1|y))+log(P(x_2|y))+...+log(P(x_3|y))+log(P(y))$$

Now, we need to come up with the prior and class conditional probabilities.

The prior probability $P(y)$ is just the frequency. 
The class conditional probability $P(x_i|y)$ is modeled as a Gaussian distribution in this case since the features are continuous. As a note, you can use different distributions, such as Multinomial and Bernoulli, and the algorithm is not only limited to a Gaussian distribution. 

The probability density function for the normal distribution is:

$$
P\left(x_{i} \mid y\right)=\frac{1}{\sqrt{2 \pi \sigma_{y}^{2}}} \cdot \operatorname{exp}\left(-\frac{\left(x_{i}-\mu_{y}\right)^{2}}{2 \sigma_{y}^{2}}\right)
$$

Visually, the normal distribution probability density function with varying means and standard deviations looks like: 
```{r}
#making the range
x <- seq(-5,5,length=200)
#plotting each curve 
plot(x,dnorm(x ,mean = 0, sd = sqrt(.2)), type = "l", lwd=2 , col = "blue", main = 'Normal Distribution PDF', xlim = c(-5,5), ylim = c(0,1), xlab = 'X', ylab = 'Probability Density')
curve(dnorm(x, mean = 0, sd = 1), add = TRUE, type = "l", lwd = 2, col= "red")
curve(dnorm(x, mean = 0, sd = sqrt(5)), add = TRUE, type = "l", lwd = 2, col = "brown")
curve(dnorm(x, mean = -2, sd = sqrt(.5)), add = TRUE, type = "l", lwd = 2, col = "green")
```

Now we have all of the formulas to code naive Bayes from scratch.

-----

# Coding the algorithm

First, let's load the packages we will be using.
```{r warning=FALSE, message=FALSE}
library(tidyverse)
```

Prior to coding the algorithm, let's create a dataset that will be used to test it. For this, we will modify the built-in `iris` dataset. For the outcome, we will only be using `setosa` and `versicolor`. To keep it simple, from $x$ variables, we will only be using `Sepal.Length` and `Petal.Length`. All of the datasets used here will be in the `Data` folder. 
```{r}
#loading the iris data and making it binary
df <- iris[iris$Species != "virginica", ]
#selecting the x variables
X <- df[, c("Sepal.Length", "Petal.Length")]
#selecting the outcome variable and converting it into setosa = 1 and versicolor = 2
y <- ifelse(df$Species == "setosa", 1, 2)
#plotting 
ggplot(df, aes(x = Sepal.Length, y = Petal.Length)) + 
        geom_point(aes(colour = Species, shape = Species), size = 3) +
        xlab("Sepal Length") + 
        ylab("Petal Length") + 
        ggtitle("Species vs Sepal and Petal Lengths")
```

Now that we have prepared the data that we will use, we can code the naive Bayes classifier. The naive Bayes algorithm looks like:

* Calculate the mean and standard deviation for each class for each of the features. So, since there are 2 classes ($[1,2]$) and 2 features (`Sepal.Length` and `Petal.Length`), we we will get a total of 8 parameters (4 means and 4 standard deviations).

* Calculate the prior probabilities for each class. This is simply the proportion of each class out of all the possible classes in the sample in the sample. 

* Calculate the likelihoods/class conditional probabilities for each class. Since the features are continuous, we are assuming a Gaussian normal distribution, and since we are assuming independence, we can multiply the Gaussian distributions. So, we will have a mean and standard deviation for feature 1 in class 1 that will be multiplied by a mean and standard deviation for feature 2 in class 1. This is done for all classes. 

*NOTE*: If you are taking the log, then the distributions are added together, not multiplied.  

* Calculate the posterior probability for a given observation using the Gaussian distributions we multiplied (or added if we took the log) together multiplied (or added) by/to the prior probability. 

* Assign the observation to the class with the highest probability. So, in this case, we will create 2 probabilities and the observation will be assigned to class with the higher of the 2 probabilities. If there are 3 classes, then there will be 3 probabilities and the observation will be assigned to the class with the highest of the 3 probabilities. 
```{r}
naive_bayes <- function(X, y){
  
  #converting X to a matrix
  X <- as.matrix(X)
  
  #defining the number of features
  n_feat <- ncol(X)
  #defining the number observations
  n_samp <- nrow(X)
  #defining the number of classes in the outcome
  n_class <- length(unique(y))
  
  #initializing the mean
  mean_ini <- matrix(data = 0, nrow = n_class, ncol = n_feat) #for each class we need means for each feature
  #initializing the variance 
  sd_ini <- matrix(data = 0, nrow = n_class, ncol = n_feat) #for each class we need sd for each feature
  #initializing the priors
  priors_ini <- rep(0, n_class) #for each class we want a prior probability 

  #calculating the mean and sd for each class
  for(i in 1:n_feat){
    #calculating the mean for each class
    mean_ini[, i] <- unlist(aggregate(X[, i] ~ y, X, mean)[2], use.names = F)
    #calculating the variance for each class
    sd_ini[, i] <- unlist(aggregate(X[, i] ~ y, X, sd)[2], use.names = F)
  }
  #calculating the prior for each class
  for(i in 1:n_class){
    priors_ini[i] <- length(y[y == i])/length(y) #the proportion of each flower class out of all the flowers from the sample 
  }
  
  #initializing the likelihood 
  likeli_ini <- matrix(data = 0, nrow = n_samp, ncol = n_class)
  
  #calculating the log likelihoods for each observation for each class   
  for(i in 1:n_samp){
    for(j in 1:n_class){
      likeli_ini[i, j] <- sum(dnorm(X[i, ], mean_ini[j, ], sd_ini[j, ], log = T))
    }
  }
  
  #calculating the posterior probabilities for each class
  posteriors <- likeli_ini + log(priors_ini)
  
  #assigning the classes based on the highest probability
  class_assignment <- apply(X = posteriors, MARGIN = 1, FUN = which.max)
  
  #returning the output as a data frame with original information and class assignment
  return(as.data.frame(cbind(X, y, class_assignment)))
  
}
```

Now, let's see if our function works.
```{r}
#running the function
out <- naive_bayes(X = X, y = y)
#looking at the first 6 observations
head(out)
```

Now that we know our function runs, let's see if it correctly assigned the observations to their respective classes via tables and a plot.
```{r}
#table of original output
table(y)
#table of class assignments from our algorithm
table(out$class_assignment)

#plotting
ggplot(out, aes(x = Sepal.Length, y = Petal.Length)) + 
        geom_point(aes(colour = as.factor(class_assignment), shape = as.factor(class_assignment)), size = 3) +
        xlab("Sepal Length") + 
        ylab("Petal Length") + 
        ggtitle("Class Assignment vs Sepal and Petal Lengths")
```

The algorithm works perfectly. Now, let's do one more test run to see if it works correctly with more than 2 features. This is just because I am curious and wanted to write code that can generalize to other types of data sets and not just this one.
```{r}
#selecting the x variables
X <- df[, 1:4]
#selecting the outcome variable and converting it into setosa = 1 and versicolor = 0
y <- ifelse(df$Species == "setosa", 1, 2)

#running the function
out2 <- naive_bayes(X = X, y = y)
#looking at the first 6 observations
head(out2)

#table of original output
table(y)
#table of class assignments from our algorithm
table(out2$class_assignment)

#plotting
ggplot(out2, aes(x = Sepal.Length, y = Petal.Length)) + 
        geom_point(aes(colour = as.factor(class_assignment), shape = as.factor(class_assignment)), size = 3) +
        xlab("Sepal Length") + 
        ylab("Petal Length") + 
        ggtitle("Class Assignment vs Sepal and Petal Lengths")
```

The algorithm does generalize to more than 2 features. Finally, let's see if our code extends to more than 2 classes.
```{r}
#loading the iris data
df <- iris
#selecting the x variables
X <- df[, 1:4]
#selecting the outcome variable and converting it into setosa = 1, versicolor = 2, virginica = 3
y <- with(df, ifelse(Species == "setosa", 1, 
          ifelse(Species == "versicolor", 2, 3)))
#plotting 
ggplot(df, aes(x = Sepal.Length, y = Petal.Length)) + 
        geom_point(aes(colour = Species, shape = Species), size = 3) +
        xlab("Sepal Length") + 
        ylab("Petal Length") + 
        ggtitle("Species vs Sepal and Petal Lengths")

#running the function
out3 <- naive_bayes(X = X, y = y)
#looking at the first 6 observations
head(out3)

#table of original output
table(y)
#table of class assignments from our algorithm
table(out3$class_assignment)

#plotting
ggplot(out3, aes(x = Sepal.Length, y = Petal.Length)) + 
        geom_point(aes(colour = as.factor(class_assignment), shape = as.factor(class_assignment)), size = 3) +
        xlab("Sepal Length") + 
        ylab("Petal Length") + 
        ggtitle("Class Assignment vs Sepal and Petal Lengths")
```

The algorithm extends beyond 2 classes. So, we have successfully coded naive Bayes from scratch.

-----

# End of document

-----

```{r}
sessionInfo()
```

