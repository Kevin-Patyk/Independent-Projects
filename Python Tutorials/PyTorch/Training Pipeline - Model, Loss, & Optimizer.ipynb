{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ac14e7b",
   "metadata": {},
   "source": [
    "# Training Pipeline - Model, Loss, & Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11454476",
   "metadata": {},
   "source": [
    "In this steps, we will perform steps 3 and 4, as outlined in the previous tutorial. As a reminder, the steps are: \n",
    "\n",
    "**Step 1**:\n",
    "\n",
    "* Prediction: Manually\n",
    "* Gradients Computation: Manually\n",
    "* Loss Computation: Manually\n",
    "* Parameters Updates: Manually\n",
    "\n",
    "**Step 2**:\n",
    "\n",
    "* Prediction: Manually\n",
    "* Gradients Computation: `Autograd`\n",
    "* Loss Computation: Manually\n",
    "* Parameters Updates: Manually\n",
    "\n",
    "**Step 3**:\n",
    "\n",
    "* Prediction: Manually\n",
    "* Gradients Computation: `Autograd`\n",
    "* Loss Computation: `Pytorch Loss`\n",
    "* Parameters Updates: `Pytorch Optimizer`\n",
    "\n",
    "**Step 4**:\n",
    "\n",
    "* Prediction: `Pytorch Model`\n",
    "* Gradients Computation: `Autograd`\n",
    "* Loss Computation: `Pytorch Loss`\n",
    "* Parameters Updates: `Pytorch Optimizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b0fb0e",
   "metadata": {},
   "source": [
    "So, we are going to replace the manually computed elements with only `pytorch` code. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda4cfec",
   "metadata": {},
   "source": [
    "In `pytorch`, the general training pipeline looks like:\n",
    "\n",
    "* Design the model (input size, output size, forward pass)\n",
    "* Construct the loss and optimizer\n",
    "* Training loop\n",
    "    * Forward pass: Compute prediction\n",
    "    * Backward pass: Compute the gradients\n",
    "    * Update our parameters \n",
    "\n",
    "We will iterate this many times until it converges or reaches a certain number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43b0a3c",
   "metadata": {},
   "source": [
    "First, let's import `pytorch` and `torch.nn`. This will allow us to replace the loss and optimization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d7bef36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bd84bb",
   "metadata": {},
   "source": [
    "Let's define the training samples and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc10ce1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making training samples with pytorch\n",
    "X = torch.tensor([1,2,3,4], dtype = torch.float32)\n",
    "y = torch.tensor([2,4,6,8], dtype = torch.float32)\n",
    "\n",
    "#initializing the weight at 0 - we need to see requires_grad = True\n",
    "w = torch.tensor(0.0, dtype = torch.float32, requires_grad = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c7c1b2",
   "metadata": {},
   "source": [
    "Let's define the model prediction/forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba14f72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward pass\n",
    "def forward(x):\n",
    "    return w * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8b1cf3",
   "metadata": {},
   "source": [
    "In the previous video, we defined the loss as below, but here we will not need to do it because we will be replacing the manual calculation.\n",
    "\n",
    "`def loss(y, y_predicted):\n",
    "    return ((y_predicted - y)**2).mean()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33472c7",
   "metadata": {},
   "source": [
    "Now, let's making the training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d920216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: w = 0.300, loss = 30.00000000\n",
      "epoch 11: w = 1.665, loss = 1.16278565\n",
      "epoch 21: w = 1.934, loss = 0.04506890\n",
      "epoch 31: w = 1.987, loss = 0.00174685\n",
      "epoch 41: w = 1.997, loss = 0.00006770\n",
      "epoch 51: w = 1.999, loss = 0.00000262\n",
      "epoch 61: w = 2.000, loss = 0.00000010\n",
      "epoch 71: w = 2.000, loss = 0.00000000\n",
      "epoch 81: w = 2.000, loss = 0.00000000\n",
      "epoch 91: w = 2.000, loss = 0.00000000\n",
      " Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "#making training samples with pytorch\n",
    "X = torch.tensor([1,2,3,4], dtype = torch.float32)\n",
    "y = torch.tensor([2,4,6,8], dtype = torch.float32)\n",
    "\n",
    "#initializing the weight at 0 - we need to see requires_grad = True\n",
    "w = torch.tensor(0.0, dtype = torch.float32, requires_grad = True)\n",
    "\n",
    "#learning rate\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "#we still need to define the loss - we can use a loss that is provided by pytorch\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "#we also need to define an optimizer from pytorch - we need to input the parameters to optimize as a [list] and the lr\n",
    "optimizer = torch.optim.SGD([w], lr = learning_rate)\n",
    "\n",
    "#now we do the training loop\n",
    "for epoch in range(n_iters):\n",
    "    \n",
    "    #prediction = forward pass\n",
    "    y_pred = forward(X)\n",
    "    \n",
    "    #loss - this still needs to be defined because it is a callable function, which takes the actual values \n",
    "    # (y) and the predicted values (y_pred)\n",
    "    l = loss(y, y_pred)\n",
    "    \n",
    "    #gradients with respect to w = backward pass \n",
    "    l.backward() #gradient of the loss with respect to w\n",
    "    \n",
    "    #we do not need to manually update our weights anymore, so we can simply say:\n",
    "    optimizer.step() #this does an optimization step\n",
    "    \n",
    "    #we still have to empty the gradient after our optimization step\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    #printing some information\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "        \n",
    "print(f' Prediction after training: f(5) = {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f79fb31",
   "metadata": {},
   "source": [
    "As we can see, our prediction is good after the training. Now, let's continue with step 4, replacing our manually calcuated forward method with a `pytorch` automatic calculation. \n",
    "\n",
    "Now, we do not need our forward pass, which we defined as:\n",
    "\n",
    "`def forward(x):\n",
    "    return w * x`\n",
    "    \n",
    "And we no longer need to define the weight `w` because `pytorch` knows the parameters and will initialize them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aed5da35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Prediction after training: f(5) = 0.495\n",
      "epoch 1: w = 0.232, loss = 24.61823654\n",
      "epoch 11: w = 1.375, loss = 0.87271494\n",
      "epoch 21: w = 1.569, loss = 0.24463740\n",
      "epoch 31: w = 1.610, loss = 0.21546268\n",
      "epoch 41: w = 1.626, loss = 0.20253506\n",
      "epoch 51: w = 1.637, loss = 0.19073656\n",
      "epoch 61: w = 1.648, loss = 0.17963444\n",
      "epoch 71: w = 1.659, loss = 0.16917877\n",
      "epoch 81: w = 1.669, loss = 0.15933174\n",
      "epoch 91: w = 1.679, loss = 0.15005785\n",
      " Prediction after training: f(5) = 9.356\n"
     ]
    }
   ],
   "source": [
    "#making training samples with pytorch\n",
    "#X now has to be a 2D array & where the number of rows is the number of samples and the columns are features - in this example\n",
    "#X is 4 samples and 1 feature \n",
    "X = torch.tensor([[1],[2],[3],[4]], dtype = torch.float32)\n",
    "y = torch.tensor([[2],[4],[6],[8]], dtype = torch.float32)\n",
    "\n",
    "#we will define the number of samples and features - we have four samples and one feature\n",
    "n_samples = list(X.shape)[0]\n",
    "n_features = list(X.shape)[1]\n",
    "\n",
    "input_size = n_features #this is 1 \n",
    "output_size = n_features #this is 1\n",
    "\n",
    "#defining the model for the forward pass\n",
    "model = nn.Linear(in_features = input_size, out_features = output_size) #this needs an input and output size of our features -\n",
    "#for this, we need to modify our X and y - they  need to have a different shape - we did this above\n",
    "\n",
    "#test tensor \n",
    "X_test = torch.tensor([5], dtype = torch.float32)\n",
    "\n",
    "#if we want the prediction, we can simply say below, but now we cannot have a float value and it must be a float value\n",
    "print(f' Prediction after training: f(5) = {model(X_test).item():.3f}') #we can call .item() to get the actual float value\n",
    "\n",
    "#learning rate\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "#we still need to define the loss - we can use a loss that is provided by pytorch\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "#since we are doing a forward pass with pytorch, we do not have w defined, so we need to modify this \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "#now we do the training loop\n",
    "for epoch in range(n_iters):\n",
    "    \n",
    "    #prediction = forward pass - changing this to the model\n",
    "    y_pred = model(X)\n",
    "    \n",
    "    #loss - this still needs to be defined because it is a callable function, which takes the actual values \n",
    "    # (y) and the predicted values (y_pred)\n",
    "    l = loss(y, y_pred)\n",
    "    \n",
    "    #gradients with respect to w = backward pass \n",
    "    l.backward() #gradient of the loss with respect to w\n",
    "    \n",
    "    #we do not need to manually update our weights anymore, so we can simply say:\n",
    "    optimizer.step() #this does an optimization step\n",
    "    \n",
    "    #we still have to empty the gradient after our optimization step\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    #if we want the prediction, we can simply say below, but now we cannot have a float value and it must be a float value\n",
    "    if epoch % 10 == 0:\n",
    "        [w, b] = model.parameters() #this will unpack the parameters\n",
    "        print(f'epoch {epoch + 1}: w = {w[0][0].item():.3f}, loss = {l:.8f}') #we can call .item() to get the actual float value\n",
    "        \n",
    "print(f' Prediction after training: f(5) = {model(X_test).item():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00b682d",
   "metadata": {},
   "source": [
    "As we can see, the prediction is a bit off. This may be due to the fact that the weights are randomly intialized. We can change how the weight is initialized, play with the number of iterations, and also change the optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7f8f24",
   "metadata": {},
   "source": [
    "As a note, this is how the tensor `X` look before and after it is reshaped. Also, we display how to get the shape of a tensor, both rows and columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "519c673b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4.])\n",
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [4.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4, 1]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#old\n",
    "X = torch.tensor([1,2,3,4], dtype = torch.float32)\n",
    "print(X)\n",
    "\n",
    "#new\n",
    "X = torch.tensor([[1],[2],[3],[4]], dtype = torch.float32)\n",
    "print(X)\n",
    "\n",
    "#to see the shape of a tensor, you can do:\n",
    "list(X.shape)\n",
    "\n",
    "#or\n",
    "[*X.shape]\n",
    "\n",
    "#or\n",
    "[*X.size()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc3b9c6",
   "metadata": {},
   "source": [
    "In this example, we used the model:\n",
    "\n",
    "`model = nn.Linear(in_features = input_size, out_features = output_size)`\n",
    "\n",
    "But, we may need a custom model. In this case, you can write a custom model. We will do so in the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6eb64934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Prediction after training: f(5) = 2.518\n",
      "epoch 1: w = 0.699, loss = 16.40454865\n",
      "epoch 11: w = 1.631, loss = 0.47620538\n",
      "epoch 21: w = 1.786, loss = 0.06108698\n",
      "epoch 31: w = 1.815, loss = 0.04750837\n",
      "epoch 41: w = 1.824, loss = 0.04448386\n",
      "epoch 51: w = 1.830, loss = 0.04188794\n",
      "epoch 61: w = 1.835, loss = 0.03944966\n",
      "epoch 71: w = 1.840, loss = 0.03715352\n",
      "epoch 81: w = 1.845, loss = 0.03499096\n",
      "epoch 91: w = 1.849, loss = 0.03295427\n",
      " Prediction after training: f(5) = 9.698\n"
     ]
    }
   ],
   "source": [
    "#making training samples with pytorch\n",
    "#X now has to be a 2D array & where the number of rows is the number of samples and the columns are features - in this example\n",
    "#X is 4 samples and 1 feature \n",
    "X = torch.tensor([[1],[2],[3],[4]], dtype = torch.float32)\n",
    "y = torch.tensor([[2],[4],[6],[8]], dtype = torch.float32)\n",
    "\n",
    "#we will define the number of samples and features - we have four samples and one feature\n",
    "n_samples = list(X.shape)[0]\n",
    "n_features = list(X.shape)[1]\n",
    "\n",
    "input_size = n_features #this is 1 \n",
    "output_size = n_features #this is 1\n",
    "\n",
    "#defining the model for the forward pass\n",
    "#model = nn.Linear(in_features = input_size, out_features = output_size) #this needs an input and output size of our features -\n",
    "#for this, we need to modify our X and y - they  need to have a different shape - we did this above\n",
    "\n",
    "#writing the custom linear regression model - we have to derive this from nn.Module\n",
    "class LinearRegression(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        #define layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return(self.lin(x))\n",
    "\n",
    "#now, making a model \n",
    "model = LinearRegression(input_size, output_size)\n",
    "\n",
    "#test tensor \n",
    "X_test = torch.tensor([5], dtype = torch.float32)\n",
    "\n",
    "#if we want the prediction, we can simply say below, but now we cannot have a float value and it must be a float value\n",
    "print(f' Prediction after training: f(5) = {model(X_test).item():.3f}') #we can call .item() to get the actual float value\n",
    "\n",
    "#learning rate\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "#we still need to define the loss - we can use a loss that is provided by pytorch\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "#since we are doing a forward pass with pytorch, we do not have w defined, so we need to modify this \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "#now we do the training loop\n",
    "for epoch in range(n_iters):\n",
    "    \n",
    "    #prediction = forward pass - changing this to the model\n",
    "    y_pred = model(X)\n",
    "    \n",
    "    #loss - this still needs to be defined because it is a callable function, which takes the actual values \n",
    "    # (y) and the predicted values (y_pred)\n",
    "    l = loss(y, y_pred)\n",
    "    \n",
    "    #gradients with respect to w = backward pass \n",
    "    l.backward() #gradient of the loss with respect to w\n",
    "    \n",
    "    #we do not need to manually update our weights anymore, so we can simply say:\n",
    "    optimizer.step() #this does an optimization step\n",
    "    \n",
    "    #we still have to empty the gradient after our optimization step\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    #if we want the prediction, we can simply say below, but now we cannot have a float value and it must be a float value\n",
    "    if epoch % 10 == 0:\n",
    "        [w, b] = model.parameters() #this will unpack the parameters\n",
    "        print(f'epoch {epoch + 1}: w = {w[0][0].item():.3f}, loss = {l:.8f}') #we can call .item() to get the actual float value\n",
    "        \n",
    "print(f' Prediction after training: f(5) = {model(X_test).item():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a53db5",
   "metadata": {},
   "source": [
    "Now we have replaced all of our manual calculations with `pytorch` calculations. However, we still have to design the model and know which loss and optimizer we have to use. We do not have to worry about the underlying algorithms anymore, though. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
