{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec57a4b2",
   "metadata": {},
   "source": [
    "# Gradient Descent with Autograd & Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4921e4",
   "metadata": {},
   "source": [
    "In this tutorial, we will do a concrete example of how we optimize our model with automatic gradient computation using the `pytroch` `autograd` package. There will be 4 steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d0e369",
   "metadata": {},
   "source": [
    "**Step 1**:\n",
    "\n",
    "* Prediction: Manually\n",
    "* Gradients Computation: Manually\n",
    "* Loss Computation: Manually\n",
    "* Parameters Updates: Manually\n",
    "\n",
    "**Step 2**:\n",
    "\n",
    "* Prediction: Manually\n",
    "* Gradients Computation: `Autograd`\n",
    "* Loss Computation: Manually\n",
    "* Parameters Updates: Manually\n",
    "\n",
    "**Step 3**:\n",
    "\n",
    "* Prediction: Manually\n",
    "* Gradients Computation: `Autograd`\n",
    "* Loss Computation: `Pytorch Loss`\n",
    "* Parameters Updates: `Pytorch Optimizer`\n",
    "\n",
    "**Step 4**:\n",
    "\n",
    "* Prediction: `Pytorch Model`\n",
    "* Gradients Computation: `Autograd`\n",
    "* Loss Computation: `Pytorch Loss`\n",
    "* Parameters Updates: `Pytorch Optimizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91477dc0",
   "metadata": {},
   "source": [
    "Once we understand the manual process, we will move on to using `pytorch` to do everything for us. This tutorial will cover steps 1 and 2. In the next video, we will perform steps 3 and 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1515e6",
   "metadata": {},
   "source": [
    "First, we will import `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "678c0609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395c5f24",
   "metadata": {},
   "source": [
    "We are using linear regression, so a linear combination of ours weights *w* multiplied by an input *x*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e178479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 3. 4.]\n",
      "[2. 4. 6. 8.]\n"
     ]
    }
   ],
   "source": [
    "# f = w * x\n",
    "\n",
    "# our weight will be 2, f = 2 * w\n",
    "\n",
    "#making training samples\n",
    "X = np.array([1,2,3,4], dtype = np.float32)\n",
    "y = np.array([2,4,6,8], dtype = np.float32)\n",
    "\n",
    "#initializing the weight at 0\n",
    "w = 0\n",
    "\n",
    "#inspect the training samples\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb0fa1a",
   "metadata": {},
   "source": [
    "Now, we will make our model predictions. We will define a function. This will be called `forward` to follow the conventions `pytorch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b71c9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    return w * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22bbeb0",
   "metadata": {},
   "source": [
    "Now, we will define our loss function. The loss depends on y and y predicted (model output). The loss we are using is the mean squared error (MSE) in the case of linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e96c710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted - y)**2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b200af68",
   "metadata": {},
   "source": [
    "Now we have to calculate the gradient of the loss with respect to our parameters. The MSE formula is:\n",
    "\n",
    "$$ \\frac{1}{N} \\cdot (w \\cdot x - y)^2 $$\n",
    "\n",
    "The derivative of this function with respect to w is:\n",
    "\n",
    "$$ \\frac{1}{N} \\cdot 2x \\cdot (w\\cdot x- y) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "437ee72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x, y, y_predicted):\n",
    "    return np.dot(2*x, y_predicted - y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5df2368",
   "metadata": {},
   "source": [
    "Now, let's print our prediction before the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56856e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Prediction before training: f(5) = 0.000\n"
     ]
    }
   ],
   "source": [
    "print(f' Prediction before training: f(5) = {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31ac95e",
   "metadata": {},
   "source": [
    "Now, let's start our training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "157f2465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: w = 1.200, loss = 30.00000000, gradient = -120.000\n",
      "epoch 2: w = 1.680, loss = 4.79999924, gradient = -48.000\n",
      "epoch 3: w = 1.872, loss = 0.76800019, gradient = -19.200\n",
      "epoch 4: w = 1.949, loss = 0.12288000, gradient = -7.680\n",
      "epoch 5: w = 1.980, loss = 0.01966083, gradient = -3.072\n",
      "epoch 6: w = 1.992, loss = 0.00314570, gradient = -1.229\n",
      "epoch 7: w = 1.997, loss = 0.00050332, gradient = -0.492\n",
      "epoch 8: w = 1.999, loss = 0.00008053, gradient = -0.197\n",
      "epoch 9: w = 1.999, loss = 0.00001288, gradient = -0.079\n",
      "epoch 10: w = 2.000, loss = 0.00000206, gradient = -0.031\n",
      " Prediction after training: f(5) = 9.999\n"
     ]
    }
   ],
   "source": [
    "#making training samples\n",
    "X = np.array([1,2,3,4], dtype = np.float32)\n",
    "y = np.array([2,4,6,8], dtype = np.float32)\n",
    "\n",
    "#initializing the weight at 0\n",
    "w = 0\n",
    "\n",
    "#learning rate\n",
    "learning_rate = 0.01\n",
    "n_iters = 10\n",
    "\n",
    "#now we do the training loop\n",
    "for epoch in range(n_iters):\n",
    "    \n",
    "    #prediction = forward\n",
    "    y_pred = forward(X)\n",
    "    \n",
    "    #calculate the loss\n",
    "    l = loss(y, y_pred)\n",
    "    \n",
    "    #gradients with respect to w\n",
    "    dw = gradient(X, y, y_pred)\n",
    "    \n",
    "    #update our weights - we go in the negative direction of the gradient\n",
    "    w -= learning_rate * dw\n",
    "    \n",
    "    #printing some information\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}, gradient = {dw:.3f}')\n",
    "        \n",
    "print(f' Prediction after training: f(5) = {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c87653d",
   "metadata": {},
   "source": [
    "We see that with each training step, it increases our weight and decreases our loss. It gets better with every step. After training the prediction is 9.99, which is close to 10, as it should be. This is because $5 \\cdot 2 = 10$. If we increase the number of iterations in the training loop, we will get closer to 10.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc2f974b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: w = 1.200, loss = 30.00000000, gradient = -120.000\n",
      "epoch 3: w = 1.872, loss = 0.76800019, gradient = -19.200\n",
      "epoch 5: w = 1.980, loss = 0.01966083, gradient = -3.072\n",
      "epoch 7: w = 1.997, loss = 0.00050332, gradient = -0.492\n",
      "epoch 9: w = 1.999, loss = 0.00001288, gradient = -0.079\n",
      "epoch 11: w = 2.000, loss = 0.00000033, gradient = -0.013\n",
      "epoch 13: w = 2.000, loss = 0.00000001, gradient = -0.002\n",
      "epoch 15: w = 2.000, loss = 0.00000000, gradient = -0.000\n",
      "epoch 17: w = 2.000, loss = 0.00000000, gradient = -0.000\n",
      "epoch 19: w = 2.000, loss = 0.00000000, gradient = -0.000\n",
      " Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "#making training samples\n",
    "X = np.array([1,2,3,4], dtype = np.float32)\n",
    "y = np.array([2,4,6,8], dtype = np.float32)\n",
    "\n",
    "#initializing the weight at 0\n",
    "w = 0\n",
    "\n",
    "#learning rate\n",
    "learning_rate = 0.01\n",
    "n_iters = 20\n",
    "\n",
    "#now we do the training loop\n",
    "for epoch in range(n_iters):\n",
    "    \n",
    "    #prediction = forward\n",
    "    y_pred = forward(X)\n",
    "    \n",
    "    #calculate the loss\n",
    "    l = loss(y, y_pred)\n",
    "    \n",
    "    #gradients with respect to w\n",
    "    dw = gradient(X, y, y_pred)\n",
    "    \n",
    "    #update our weights - we go in the negative direction of the gradient\n",
    "    w -= learning_rate * dw\n",
    "    \n",
    "    #printing some information\n",
    "    if epoch % 2 == 0:\n",
    "        print(f'epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}, gradient = {dw:.3f}')\n",
    "        \n",
    "print(f' Prediction after training: f(5) = {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14537cf",
   "metadata": {},
   "source": [
    "As we can see, by the iteration 11, the loss is almost completely 0 and the `w` is 2, as it should be. By iteration 15, the loss is completely 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cc06c1",
   "metadata": {},
   "source": [
    "In this first example we did everything manually. Now, let's replace the gradient calculation with the `autograd` package from `pytorch`. First, we need to import `torch`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28d87c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f5e168",
   "metadata": {},
   "source": [
    "Now, `X` and `y` need to become tensors. `w` also needs to become a tensor and we need to set the argument `requires_grad = True` since we need to calculate the gradient/derivative of the loss with respect to `w`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49ae27a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making training samples with pytorch\n",
    "X = torch.tensor([1,2,3,4], dtype = torch.float32)\n",
    "y = torch.tensor([2,4,6,8], dtype = torch.float32)\n",
    "\n",
    "#initializing the weight at 0 - we need to see requires_grad = True\n",
    "w = torch.tensor(0.0, dtype = torch.float32, requires_grad = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3614a404",
   "metadata": {},
   "source": [
    "The forward pass function will still be the same and the loss function will still be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b6c7397",
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward pass\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "#loss function \n",
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted - y)**2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc08615",
   "metadata": {},
   "source": [
    "Now, in our training loop, the forward pass will be the same and the loss is the same. However, we the gradient, which is equal to the backward pass, will be different, as we will not calculate it manually; rather, we will use the `autograd` package from `pytorch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "574846d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: w = 0.300, loss = 30.00000000, gradient = 0.000\n",
      "epoch 3: w = 0.772, loss = 15.66018772, gradient = 0.000\n",
      "epoch 5: w = 1.113, loss = 8.17471695, gradient = 0.000\n",
      "epoch 7: w = 1.359, loss = 4.26725292, gradient = 0.000\n",
      "epoch 9: w = 1.537, loss = 2.22753215, gradient = 0.000\n",
      "epoch 11: w = 1.665, loss = 1.16278565, gradient = 0.000\n",
      "epoch 13: w = 1.758, loss = 0.60698116, gradient = 0.000\n",
      "epoch 15: w = 1.825, loss = 0.31684780, gradient = 0.000\n",
      "epoch 17: w = 1.874, loss = 0.16539653, gradient = 0.000\n",
      "epoch 19: w = 1.909, loss = 0.08633806, gradient = 0.000\n",
      " Prediction after training: f(5) = 9.612\n"
     ]
    }
   ],
   "source": [
    "#making training samples with pytorch\n",
    "X = torch.tensor([1,2,3,4], dtype = torch.float32)\n",
    "y = torch.tensor([2,4,6,8], dtype = torch.float32)\n",
    "\n",
    "#initializing the weight at 0 - we need to see requires_grad = True\n",
    "w = torch.tensor(0.0, dtype = torch.float32, requires_grad = True)\n",
    "\n",
    "#learning rate\n",
    "learning_rate = 0.01\n",
    "n_iters = 20\n",
    "\n",
    "#now we do the training loop\n",
    "for epoch in range(n_iters):\n",
    "    \n",
    "    #prediction = forward pass\n",
    "    y_pred = forward(X)\n",
    "    \n",
    "    #calculate the loss\n",
    "    l = loss(y, y_pred)\n",
    "    \n",
    "    #gradients with respect to w = backward pass \n",
    "    l.backward() #gradient of the loss with respect to w\n",
    "    \n",
    "    #update our weights - we go in the negative direction of the gradient\n",
    "    #we have to be careful here - this operation should not be part of our gradient tracking graph \n",
    "    #it should not be part of the computational graph, so we need to wrap it \n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "    \n",
    "    # we must zero the gradients - remember, whenever we call backward, it will write our gradients and accumulate them\n",
    "    # in the w.grad() attribute \n",
    "    #we want to be sure that our gradients are 0 before the next iteration\n",
    "    w.grad.zero_()\n",
    "    \n",
    "    #printing some information\n",
    "    if epoch % 2 == 0:\n",
    "        print(f'epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}, gradient = {dw:.3f}')\n",
    "        \n",
    "print(f' Prediction after training: f(5) = {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666e2ecd",
   "metadata": {},
   "source": [
    "Now we also see that it will increase our `w` and decrease our loss. Here we had 20 iterations, but it is not enough, like we saw previously. This is because backpropagation is not as exact as numerical gradient computation. Let's increase the number of iterations to 1000 and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e086c5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: w = 0.300, loss = 30.00000000, gradient = 0.000\n",
      "epoch 21: w = 1.934, loss = 0.04506890, gradient = 0.000\n",
      "epoch 41: w = 1.997, loss = 0.00006770, gradient = 0.000\n",
      "epoch 61: w = 2.000, loss = 0.00000010, gradient = 0.000\n",
      "epoch 81: w = 2.000, loss = 0.00000000, gradient = 0.000\n",
      "epoch 101: w = 2.000, loss = 0.00000000, gradient = 0.000\n",
      "epoch 121: w = 2.000, loss = 0.00000000, gradient = 0.000\n",
      "epoch 141: w = 2.000, loss = 0.00000000, gradient = 0.000\n",
      "epoch 161: w = 2.000, loss = 0.00000000, gradient = 0.000\n",
      "epoch 181: w = 2.000, loss = 0.00000000, gradient = 0.000\n",
      "epoch 201: w = 2.000, loss = 0.00000000, gradient = 0.000\n",
      "epoch 221: w = 2.000, loss = 0.00000000, gradient = 0.000\n",
      "epoch 241: w = 2.000, loss = 0.00000000, gradient = 0.000\n",
      "epoch 261: w = 2.000, loss = 0.00000000, gradient = 0.000\n",
      "epoch 281: w = 2.000, loss = 0.00000000, gradient = 0.000\n",
      "epoch 301: w = 2.000, loss = 0.00000000, gradient = 0.000\n",
      "epoch 321: w = 2.000, loss = 0.00000000, gradient = 0.000\n",
      "epoch 341: w = 2.000, loss = 0.00000000, gradient = 0.000\n",
      "epoch 361: w = 2.000, loss = 0.00000000, gradient = 0.000\n",
      "epoch 381: w = 2.000, loss = 0.00000000, gradient = 0.000\n",
      "epoch 401: w = 2.000, loss = 0.00000000, gradient = 0.000\n",
      "epoch 421: w = 2.000, loss = 0.00000000, gradient = 0.000\n",
      "epoch 441: w = 2.000, loss = 0.00000000, gradient = 0.000\n",
      "epoch 461: w = 2.000, loss = 0.00000000, gradient = 0.000\n",
      "epoch 481: w = 2.000, loss = 0.00000000, gradient = 0.000\n",
      "epoch 501: w = 2.000, loss = 0.00000000, gradient = 0.000\n",
      "epoch 521: w = 2.000, loss = 0.00000000, gradient = 0.000\n",
      "epoch 541: w = 2.000, loss = 0.00000000, gradient = 0.000\n",
      "epoch 561: w = 2.000, loss = 0.00000000, gradient = 0.000\n",
      "epoch 581: w = 2.000, loss = 0.00000000, gradient = 0.000\n",
      "epoch 601: w = 2.000, loss = 0.00000000, gradient = 0.000\n",
      "epoch 621: w = 2.000, loss = 0.00000000, gradient = 0.000\n",
      "epoch 641: w = 2.000, loss = 0.00000000, gradient = 0.000\n",
      "epoch 661: w = 2.000, loss = 0.00000000, gradient = 0.000\n",
      "epoch 681: w = 2.000, loss = 0.00000000, gradient = 0.000\n",
      "epoch 701: w = 2.000, loss = 0.00000000, gradient = 0.000\n",
      "epoch 721: w = 2.000, loss = 0.00000000, gradient = 0.000\n",
      "epoch 741: w = 2.000, loss = 0.00000000, gradient = 0.000\n",
      "epoch 761: w = 2.000, loss = 0.00000000, gradient = 0.000\n",
      "epoch 781: w = 2.000, loss = 0.00000000, gradient = 0.000\n",
      "epoch 801: w = 2.000, loss = 0.00000000, gradient = 0.000\n",
      "epoch 821: w = 2.000, loss = 0.00000000, gradient = 0.000\n",
      "epoch 841: w = 2.000, loss = 0.00000000, gradient = 0.000\n",
      "epoch 861: w = 2.000, loss = 0.00000000, gradient = 0.000\n",
      "epoch 881: w = 2.000, loss = 0.00000000, gradient = 0.000\n",
      "epoch 901: w = 2.000, loss = 0.00000000, gradient = 0.000\n",
      "epoch 921: w = 2.000, loss = 0.00000000, gradient = 0.000\n",
      "epoch 941: w = 2.000, loss = 0.00000000, gradient = 0.000\n",
      "epoch 961: w = 2.000, loss = 0.00000000, gradient = 0.000\n",
      "epoch 981: w = 2.000, loss = 0.00000000, gradient = 0.000\n",
      " Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "#making training samples with pytorch\n",
    "X = torch.tensor([1,2,3,4], dtype = torch.float32)\n",
    "y = torch.tensor([2,4,6,8], dtype = torch.float32)\n",
    "\n",
    "#initializing the weight at 0 - we need to see requires_grad = True\n",
    "w = torch.tensor(0.0, dtype = torch.float32, requires_grad = True)\n",
    "\n",
    "#learning rate\n",
    "learning_rate = 0.01\n",
    "n_iters = 1000\n",
    "\n",
    "#now we do the training loop\n",
    "for epoch in range(n_iters):\n",
    "    \n",
    "    #prediction = forward pass\n",
    "    y_pred = forward(X)\n",
    "    \n",
    "    #calculate the loss\n",
    "    l = loss(y, y_pred)\n",
    "    \n",
    "    #gradients with respect to w = backward pass \n",
    "    l.backward() #gradient of the loss with respect to w\n",
    "    \n",
    "    #update our weights - we go in the negative direction of the gradient\n",
    "    #we have to be careful here - this operation should not be part of our gradient tracking graph \n",
    "    #it should not be part of the computational graph, so we need to wrap it \n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "    \n",
    "    # we must zero the gradients - remember, whenever we call backward, it will write our gradients and accumulate them\n",
    "    # in the w.grad() attribute \n",
    "    #we want to be sure that our gradients are 0 before the next iteration\n",
    "    w.grad.zero_()\n",
    "    \n",
    "    #printing some information\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}, gradient = {dw:.3f}')\n",
    "        \n",
    "print(f' Prediction after training: f(5) = {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5f84d4",
   "metadata": {},
   "source": [
    "As we can see, the `w` is now correct and so is the prediction. In the next video, we will complete steps 3 and 4."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
