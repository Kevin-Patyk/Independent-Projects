{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42699558",
   "metadata": {},
   "source": [
    "# Feed Forward Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f5fbc6",
   "metadata": {},
   "source": [
    "A feed forward neural network is an artificial neural network in which the connections between nodes does not form a cycle. The opposite of a feed wofard neural network is a recurrent neural network, in which certain pathways are cycled. The feed forward model is the simplest form of neurl network as information is only processed in one direction. While data may pass through multiple hidden nodes, it always moves in one direction and never backwards. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2387021",
   "metadata": {},
   "source": [
    "In this tutorial, we will use everything we learned previously and put it together using the `MNIST` dataset. The `MNIST` dataset is used for recognizing hand-written digits. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6fa841",
   "metadata": {},
   "source": [
    "Our steps for this tutorial look like: \n",
    "\n",
    "* Loading the MNIST data\n",
    "* DataLoader and Transformation\n",
    "* Multilayer Neural Net, Activation Function\n",
    "* Loss and Optimizer\n",
    "* Training Loop (Batch Training)\n",
    "* Model Evaluation - Calculating Accuracy\n",
    "* GPU Support \n",
    "\n",
    "Now, to import our modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb64f2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776276ba",
   "metadata": {},
   "source": [
    "Now, to do our device configuration. So, this will determine whether we are using a GPU or a CPU. Later, we will push our tensors to the device. This will guarantee it will run on the GPU, if it is supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afab5ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuring the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f06877",
   "metadata": {},
   "source": [
    "Now, to define hyperparameters. As a note, remember:\n",
    "\n",
    "* epoch = 1 complete forward and backward pass of ALL training samples\n",
    "* batch_size = number of training samples in one forward and backward pass\n",
    "* number of iterations = number of passes, each uses the *batch_size* number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec016b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784 #this is because our images have the size 28x28 = 784 - we will flatten the array to be a 1D tensor later\n",
    "hidden_size = 100 #can also try out different sizes here\n",
    "num_classes = 10 #have 10 different classes - digits from 0 to 9\n",
    "num_epochs = 2 #can be higher, but the training will take longer\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91916ace",
   "metadata": {},
   "source": [
    "No we will import our famous MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77a10567",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data - we are converting it to tensor and downloading it\n",
    "train_dataset = torchvision.datasets.MNIST(root = './data', train = True, \n",
    "                                           transform = transforms.ToTensor(), download = True)\n",
    "\n",
    "#testing data - we are converting it to tensor and downloading it\n",
    "test_dataset = torchvision.datasets.MNIST(root = './data', train = False, \n",
    "                                           transform = transforms.ToTensor())\n",
    "\n",
    "#train loader - will create an iterable for automatic batching\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "#test loader - will create an iterable for automatic batching \n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4307b6",
   "metadata": {},
   "source": [
    "Now, let's look at one batch of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddea2963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "#the iter() method returns an iterator of the given object\n",
    "#An iterator is an object that contains a countable number of values. \n",
    "#An iterator is an object that can be iterated upon, meaning that you can traverse through all the values.\n",
    "#It seems that, in this situation, we are just getting one batch out of the several that there are to examine it\n",
    "examples = iter(train_loader)\n",
    "\n",
    "#creating a new iterator each time and extracting the first object\n",
    "samples, labels = examples.next()\n",
    "print(samples.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d2dc44",
   "metadata": {},
   "source": [
    "So, the size is (100, 1, 28, 28). This is because the batch size is 100 (100 samples in the batch), 1 is because we only have 1 channel, and 28,28 is the image array which is 28x28. Our label is a tensor of size 100, so for each class label, we have 1 value here. \n",
    "\n",
    "Now, let's plot this to see how it's looking - this will show us the hand-written digits in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a63dc95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.imshow(samples[i][0], cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d8948b",
   "metadata": {},
   "source": [
    "Now we want to classify these digits. We are going to have a fully connected neural network with one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cc11990",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a class for neural net\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size) #linear layer - input size is input and output is hidden size\n",
    "        self.relu = nn.ReLU() #activation function\n",
    "        self.l2 = nn.Linear(hidden_size, num_classes) #second linear layer - input is hidden size and output is num classes\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x) #first linear layer\n",
    "        out = self.relu(out) #hidden layer\n",
    "        out = self.l2(out) #second linear layer \n",
    "        #we do not need an activation function (softmax because it is multiclass classification) - \n",
    "        #we will see that we will use cross-entropy loss - this will apply the softmax for us\n",
    "        return out\n",
    "\n",
    "#defining our model\n",
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dabb2cb",
   "metadata": {},
   "source": [
    "Now, we will create our loss and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad47f12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss \n",
    "criterion = nn.CrossEntropyLoss() #this will apply the softmax for us\n",
    "\n",
    "#optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fc3306",
   "metadata": {},
   "source": [
    "Now to do the training loop. As a note:\n",
    "\n",
    "* Epoch - An epoch is a single pass through our full training data(60,000 images). An epoch consists of training steps, which is nothing but the number of batches passed to the model until all the training data is covered. \n",
    "\n",
    "It could be expressed as number of training steps = number of training records/batch size, which is 60000 (training samples) / 100 (batch size) = 600 (steps) in our case. We will have 600 steps per epoch. We'll train the model for 2 epochs- the model will see the full training data exactly 2 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0250f764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "Epoch [1/2], Step [100/600], Loss: 0.1159\n",
      "Epoch [1/2], Step [200/600], Loss: 0.1017\n",
      "Epoch [1/2], Step [300/600], Loss: 0.1480\n",
      "Epoch [1/2], Step [400/600], Loss: 0.0304\n",
      "Epoch [1/2], Step [500/600], Loss: 0.0788\n",
      "Epoch [1/2], Step [600/600], Loss: 0.0992\n",
      "Epoch [2/2], Step [100/600], Loss: 0.0893\n",
      "Epoch [2/2], Step [200/600], Loss: 0.0708\n",
      "Epoch [2/2], Step [300/600], Loss: 0.0314\n",
      "Epoch [2/2], Step [400/600], Loss: 0.0374\n",
      "Epoch [2/2], Step [500/600], Loss: 0.0507\n",
      "Epoch [2/2], Step [600/600], Loss: 0.0969\n"
     ]
    }
   ],
   "source": [
    "#first define the number of total steps\n",
    "n_total_steps = len(train_loader)\n",
    "print(n_total_steps)\n",
    "\n",
    "#training loop\n",
    "\n",
    "#looping over epochs\n",
    "for epoch in range(num_epochs): \n",
    "    \n",
    "    #looping over batches\n",
    "    for i, (images, labels) in enumerate(train_loader): #enumerate function gives us the actual index\n",
    "        #first, have to reshape the images because it is 100, 1, 28, 28 and input size is 784\n",
    "        #thus our tensor needs the size 100 (batch size) and 784 (28x28)\n",
    "        images = images.reshape(-1, 28*28).to(device) #we put -1 as the first dimension so tensor can find it out \n",
    "        #automatically for us\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #forward pass \n",
    "        outputs = model(images)\n",
    "        #loss - takes the predicted values (outputs) and labels (actual values)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        #backward pass\n",
    "        optimizer.zero_grad() #to empty the values in the gradient attribute\n",
    "        #backpropagation\n",
    "        loss.backward()\n",
    "        #update step - update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0: #every 100th step we want to print information\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f7c669",
   "metadata": {},
   "source": [
    "Now, to create the test loop and evaluate the accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8120536a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 97.05 %\n"
     ]
    }
   ],
   "source": [
    "#we do not want to compute the gradient for all the steps we do\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    #loop over batches in the test samples\n",
    "    for images, labels in test_loader:\n",
    "        #now we must reshape this\n",
    "        images = images.reshape(-1, 28*28).to(device) #we put -1 as the first dimension so tensor can find it out \n",
    "        #automatically for us\n",
    "        labels = labels.to(device)\n",
    "        #calculate the predictions \n",
    "        outputs = model(images)\n",
    "        \n",
    "        #getting the actual predictions\n",
    "        _, predicted = torch.max(outputs.data, 1) #torch.max() will return the value and index, but we are interested in the index\n",
    "        #this is why we do not need the first actual value, hence the _,\n",
    "        n_samples += labels.size(0) #give us the number of samples in the current batch\n",
    "        n_correct += (predicted == labels).sum().item() #for each correct prediction, we will add +1\n",
    "    \n",
    "    #now, when the loop is done, we will calculate the total accuracy\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network on the 10000 test images: {acc} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
