{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8ea876a",
   "metadata": {},
   "source": [
    "# Gradient Calculation with Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37180a8e",
   "metadata": {},
   "source": [
    "Importing `pytorch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0862f1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d72252",
   "metadata": {},
   "source": [
    "Today we are talking about the `autograd` package which comes with `pytorch` and how we can calculate gradients with it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d96a241",
   "metadata": {},
   "source": [
    "Gradients are essential for all model optimization, so it is important to understand how to apply it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e611263f",
   "metadata": {},
   "source": [
    "Now, let's create a a tensor object which we will use to demonstrate how to use `autograd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65588369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.9524, -0.7465,  0.4141])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3868d07",
   "metadata": {},
   "source": [
    "Now, later, if we want to calculate the gradient of some function with respect to `x`, then what we have to do is specify the argument `requires_grad=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c189f53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0742, -0.8322,  0.7960], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad = True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab9f647",
   "metadata": {},
   "source": [
    "Now, when we do operations with this tensor, `pytorch` will create a so-called operational graph for us. So, let's try an operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08e4fa02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.9064, -0.4724,  0.4434], requires_grad=True)\n",
      "tensor([0.0936, 1.5276, 2.4434], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad = True)\n",
    "print(x)\n",
    "\n",
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057b7ed0",
   "metadata": {},
   "source": [
    "As we can see, `y` has the attribute `grad_fn`. This will point to a gradient function, which is `AddBackward` in this case. With this function, we can calculate the gradient of `y` with respect `x`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aea143",
   "metadata": {},
   "source": [
    "Let's see another operation in action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b97ac36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4800, -1.0948,  0.9835], requires_grad=True)\n",
      "tensor([1.5200, 0.9052, 2.9835], grad_fn=<AddBackward0>)\n",
      "tensor([ 4.6205,  1.6389, 17.8025], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad = True)\n",
    "print(x)\n",
    "\n",
    "y = x + 2\n",
    "print(y)\n",
    "\n",
    "z = y*y*2\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfb98a5",
   "metadata": {},
   "source": [
    "`z` will have the `grad_fn` attribute, but it will be `MulBackward`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce0fedf",
   "metadata": {},
   "source": [
    "And, if we apply the `mean()` function, it will change again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af9aad19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.8938,  1.2487, -1.0845], requires_grad=True)\n",
      "tensor(0.6860, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad = True)\n",
    "print(x)\n",
    "\n",
    "z = x.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5424b6b1",
   "metadata": {},
   "source": [
    "Now, when we want to calculate the gradients, we must just use the `backward()` function. In this case, it will calculate `z` with respect to `x`. After we do this, `x` will not have the gradients stored, so we can use `x.grad` to see them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "adb9439c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.4458, 0.9939, 0.4736], requires_grad=True)\n",
      "tensor([3.4458, 2.9939, 2.4736], grad_fn=<AddBackward0>)\n",
      "tensor([23.7469, 17.9269, 12.2376], grad_fn=<MulBackward0>)\n",
      "tensor(17.9705, grad_fn=<MeanBackward0>)\n",
      "tensor([4.5944, 3.9919, 3.2982])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad = True)\n",
    "print(x)\n",
    "\n",
    "y = x + 2\n",
    "print(y)\n",
    "\n",
    "z = y*y*2\n",
    "print(z)\n",
    "\n",
    "z = z.mean()\n",
    "print(z)\n",
    "\n",
    "z.backward() #dz/dx\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04852a8f",
   "metadata": {},
   "source": [
    "Now, let's see what happens if we do not specify the `required_grad = True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d0210ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.9426, -0.6766, -2.6491])\n",
      "tensor([ 2.9426,  1.3234, -0.6491])\n",
      "tensor([17.3174,  3.5030,  0.8426])\n",
      "tensor(7.2210)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m z \u001b[38;5;241m=\u001b[39m z\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(z)\n\u001b[1;32m---> 13\u001b[0m \u001b[43mz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#dz/dx\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mgrad)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3)\n",
    "print(x)\n",
    "\n",
    "y = x + 2\n",
    "print(y)\n",
    "\n",
    "z = y*y*2\n",
    "print(z)\n",
    "\n",
    "z = z.mean()\n",
    "print(z)\n",
    "\n",
    "z.backward() #dz/dx\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef31d15",
   "metadata": {},
   "source": [
    "As you can see, there was an error after removing that argument. We cannot calculate the gradient without it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfa8489",
   "metadata": {},
   "source": [
    "One thing we should also notice is that in the background, what calculating the gradient does is make a vector Jacobian product to get the gradients. We have the Jacobian matrix with the partial derivatives multiplied by a gradient vector. This will give us the final gradients we are interested in. This is called the chain rule. \n",
    "\n",
    "We should know that we must multiply it with a vector.\n",
    "\n",
    "In the previous examples, `z` is a single value, so we do not need to multiply it with anything. But, if it is not just a single value, we must take additional steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f1a5a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1869, 0.5155, 1.6582], requires_grad=True)\n",
      "tensor([2.1869, 2.5155, 3.6582], grad_fn=<AddBackward0>)\n",
      "tensor([ 9.5655, 12.6556, 26.7646], grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m z \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m*\u001b[39my\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(z)\n\u001b[1;32m---> 10\u001b[0m \u001b[43mz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#dz/dx\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mgrad)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py:166\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    162\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (inputs,) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m \\\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28mtuple\u001b[39m(inputs) \u001b[38;5;28;01mif\u001b[39;00m inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m()\n\u001b[0;32m    165\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[1;32m--> 166\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py:67\u001b[0m, in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 67\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     68\u001b[0m     new_grads\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mones_like(out, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format))\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad = True)\n",
    "print(x)\n",
    "\n",
    "y = x + 2\n",
    "print(y)\n",
    "\n",
    "z = y*y*2\n",
    "print(z)\n",
    "\n",
    "z.backward() #dz/dx\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c353c0",
   "metadata": {},
   "source": [
    "As you can see, now that we are not dealing with a scalar, we are getting an error. \n",
    "\n",
    "We have to create a vector of the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f59e019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6494,  1.0740,  0.3425], requires_grad=True)\n",
      "tensor([1.3506, 3.0740, 2.3425], grad_fn=<AddBackward0>)\n",
      "tensor([ 3.6485, 18.8986, 10.9749], grad_fn=<MulBackward0>)\n",
      "tensor([5.4026e-01, 1.2296e+01, 9.3701e-03])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad = True)\n",
    "print(x)\n",
    "\n",
    "y = x + 2\n",
    "print(y)\n",
    "\n",
    "z = y*y*2\n",
    "print(z)\n",
    "\n",
    "v = torch.tensor([0.1, 1.0, 0.001], dtype = torch.float32)\n",
    "z.backward(v) #dz/dx\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d138a3f",
   "metadata": {},
   "source": [
    "In the background we should know that these is a vector Jacobian product. Typically, the last operation will be something that creates a scalar value, so we do not need an additional vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a765e85b",
   "metadata": {},
   "source": [
    "We can prevent `pytorch` from tracking the history and calculating the `grad_fn`. For example, sometimes during our training loop and we want to update our weights, then this operation should not be part of our gradient computation. We will discuss this in the future. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3879f317",
   "metadata": {},
   "source": [
    "To prevent `pytorch` from tracking the gradient, we can do it in 3 ways: \n",
    "\n",
    "* `x.requires_grad(False)`\n",
    "* `x.detach()`\n",
    "* `with torch.no_grad()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709e04ec",
   "metadata": {},
   "source": [
    "Option 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b3fe012",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1721, -1.7907, -0.2643], requires_grad=True)\n",
      "tensor([ 0.1721, -1.7907, -0.2643])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad = True)\n",
    "print(x)\n",
    "\n",
    "x.requires_grad_(False)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df46e086",
   "metadata": {},
   "source": [
    "Option 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cbae3728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1274, -1.3130, -0.7082], requires_grad=True)\n",
      "tensor([ 0.1274, -1.3130, -0.7082])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad = True)\n",
    "print(x)\n",
    "\n",
    "y = x.detach()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af5c642",
   "metadata": {},
   "source": [
    "This will create a new tensor with the same values, but it does not require the gradient. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e76707",
   "metadata": {},
   "source": [
    "Option 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "673fc708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6808, -1.6855, -0.9310], requires_grad=True)\n",
      "tensor([1.3192, 0.3145, 1.0690])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad = True)\n",
    "print(x)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y = x + 2\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d10b4e7",
   "metadata": {},
   "source": [
    "One more very important thing to notice is that, whenever we call the `.backward` function, then the gradient for the tensor will be accumulated into the `.grad` attribute. The values will be summed up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "acf948a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad = True)\n",
    "\n",
    "for epoch in range(1):\n",
    "    model_output = (weights*3).sum()\n",
    "    \n",
    "    model_output.backward()\n",
    "    \n",
    "    print(weights.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36585d7f",
   "metadata": {},
   "source": [
    "Now, if we do another iteration, then the second `.backward` call will again accumulate the values and write then into the `.grad` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a24fa814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([6., 6., 6., 6.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad = True)\n",
    "\n",
    "for epoch in range(2):\n",
    "    model_output = (weights*3).sum()\n",
    "    \n",
    "    model_output.backward()\n",
    "    \n",
    "    print(weights.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5bfaca",
   "metadata": {},
   "source": [
    "And, if we do a third iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "79ea65d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([6., 6., 6., 6.])\n",
      "tensor([9., 9., 9., 9.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad = True)\n",
    "\n",
    "for epoch in range(3):\n",
    "    model_output = (weights*3).sum()\n",
    "    \n",
    "    model_output.backward()\n",
    "    \n",
    "    print(weights.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69281038",
   "metadata": {},
   "source": [
    "So all the values are summed up and our gradients are incorrect. Before we do the next iteration and optimization step, we must empty the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "59e1afe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad = True)\n",
    "\n",
    "for epoch in range(3):\n",
    "    model_output = (weights*3).sum()\n",
    "    \n",
    "    model_output.backward()\n",
    "    \n",
    "    print(weights.grad)\n",
    "    \n",
    "    weights.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd253725",
   "metadata": {},
   "source": [
    "Later, we will work with the `pytorch` `optimizer` optimization package. With this optimizer, we can do an optimization step and before we do the next iteration, we must call the `optimizer.zero_grad()` function. It will look something like:\n",
    "\n",
    "`weights = torch.ones(4, requires_grad = True)`\n",
    "\n",
    "`optimizer = torch.optim.SGD(weights, lr = 0.01)`\n",
    "`optimizer.step()`\n",
    "`optimizer.zero_grad()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e2f5c1",
   "metadata": {},
   "source": [
    "We will discuss optimization in more detail later. For now, we must remember that when we want to calculate gradients, we have to specify `requires_grad = True`, we can simply calculate the gradients using the `.backward()` function, and before we want to do the next iteration in the optimization steps, we must empty the gradient by calling the `.grad.zero_()`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
