{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdef5bfe",
   "metadata": {},
   "source": [
    "# Dataset & DataLoader - Batch Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab55d87",
   "metadata": {},
   "source": [
    "In this tutorial, we will look at the `pytorch` `Dataset` and `DataLoader` classes. So far, we have a dataset that we loaded somehow, such as from a `.csv` file. Then, we had our training loop that looped over the number of epochs and we optimized the model based on the whole dataset.\n",
    "\n",
    "`data = numpy.loadtxt('wine.csv')\n",
    "#training loop\n",
    "for epoch in range(1000):\n",
    "    x, y = data\n",
    "    # forward + backward + weight updates` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6562145d",
   "metadata": {},
   "source": [
    "This might be very time consuming if you did gradient calculations on the whole training dataset. One better way of doing this, which is computationally less expensive, is to do the divide the samples into smaller batches. Then, our training loop will look something like this:\n",
    "\n",
    "`#training loop\n",
    "for epoch in range(1000):\n",
    "    #loop over all batches\n",
    "    for i in range(total_batches):\n",
    "        x_batch, y_batch = ...`\n",
    "\n",
    "We do the optimization based only on those batches. However, if we use the built in the built-in `Dataset` and `DataLoader` classes from `pytorch`, it will do the batch calculations and iterations for us, so it is very easy to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8b15ea",
   "metadata": {},
   "source": [
    "Before we jump into the code, let's talk about some terms relating to batch training. \n",
    "\n",
    "* epoch = 1 complete forward and backward pass of ALL training samples\n",
    "* batch_size = number of training samples in one forward and backward pass\n",
    "* number of iterations = number of passes, each uses the *batch_size* number of samples\n",
    "\n",
    "Example: 100 samples, batch_size = 20 -> 100/20 = 5 iterations for 1 epoch. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2195f8",
   "metadata": {},
   "source": [
    "First, let's import the modules we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25b5d087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc31abf7",
   "metadata": {},
   "source": [
    "Now, let's make our dataset. This will be the dataset `wine.csv`, as provided by the tutorial. Our outcome will be type of wine, with labels 1, 2, and 3, in the first column. All the other columns are the features. Let's load this and split our columns into `X` and `y`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fa1ee45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WineDataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        #data loading \n",
    "        #will also convert to torch\n",
    "        xy = np.loadtxt('C:\\\\Users\\\\onef0\\\\Desktop\\\\PyTorch Tutorial\\\\wine.csv', delimiter = \",\", dtype = np.float32, skiprows = 1)\n",
    "        self.x = torch.from_numpy(xy[:, 1:]) #we want only the features, so all rows but all collumns excluding the first column\n",
    "        self.y = torch.from_numpy(xy[:, [0]]) #we want only the outcome, so all rows and only the first column - we had the extra \n",
    "        #brackets because it will be the n_samples, 1\n",
    "        self.n_samples = xy.shape[0] #first dimension is the number of samples\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        #dataset[0]\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        #len(dataset)\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb88715",
   "metadata": {},
   "source": [
    "Now, we will look at our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83d60831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.4230e+01, 1.7100e+00, 2.4300e+00, 1.5600e+01, 1.2700e+02, 2.8000e+00,\n",
      "        3.0600e+00, 2.8000e-01, 2.2900e+00, 5.6400e+00, 1.0400e+00, 3.9200e+00,\n",
      "        1.0650e+03]) tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "dataset = WineDataset()\n",
    "\n",
    "#look at the first sample\n",
    "first_data = dataset[0]\n",
    "\n",
    "#lets unpack this into features and labels\n",
    "features, labels = first_data\n",
    "print(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7946c456",
   "metadata": {},
   "source": [
    "So, this is how we get the dataset, now let's get a `DataLoader`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86633b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now, we will make a dataloader with batch size of 2 and also shuffling the dataset\n",
    "dataloader = DataLoader(dataset = dataset, batch_size = 4, shuffle = True)\n",
    "    #num_workers = 2 uses multiple subprocesses so can make loading faster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ba580c",
   "metadata": {},
   "source": [
    "Now, let's see how we can use this `DataLoader` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcd052be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2370e+01, 1.0700e+00, 2.1000e+00, 1.8500e+01, 8.8000e+01, 3.5200e+00,\n",
      "         3.7500e+00, 2.4000e-01, 1.9500e+00, 4.5000e+00, 1.0400e+00, 2.7700e+00,\n",
      "         6.6000e+02],\n",
      "        [1.3720e+01, 1.4300e+00, 2.5000e+00, 1.6700e+01, 1.0800e+02, 3.4000e+00,\n",
      "         3.6700e+00, 1.9000e-01, 2.0400e+00, 6.8000e+00, 8.9000e-01, 2.8700e+00,\n",
      "         1.2850e+03],\n",
      "        [1.2080e+01, 1.3300e+00, 2.3000e+00, 2.3600e+01, 7.0000e+01, 2.2000e+00,\n",
      "         1.5900e+00, 4.2000e-01, 1.3800e+00, 1.7400e+00, 1.0700e+00, 3.2100e+00,\n",
      "         6.2500e+02],\n",
      "        [1.3050e+01, 5.8000e+00, 2.1300e+00, 2.1500e+01, 8.6000e+01, 2.6200e+00,\n",
      "         2.6500e+00, 3.0000e-01, 2.0100e+00, 2.6000e+00, 7.3000e-01, 3.1000e+00,\n",
      "         3.8000e+02]]) tensor([[2.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [2.]])\n"
     ]
    }
   ],
   "source": [
    "#first, we can convert it to a iterator\n",
    "dataiter = iter(dataloader)\n",
    "data = dataiter.next()\n",
    "features, labels = data\n",
    "\n",
    "print(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc06e497",
   "metadata": {},
   "source": [
    "In the above example, we have 4 batches, that is why there are 4 different feature vectors. For each vector, the class, so 4 output labels. \n",
    "\n",
    "We also can iterate over the whole data loader and not only get the next item. Let's do a dummy training loop. First, let's define some things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fd90b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178 45\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2 \n",
    "total_samples = len(dataset)\n",
    "n_iterations = math.ceil(total_samples/4)\n",
    "print(total_samples, n_iterations) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8acc056",
   "metadata": {},
   "source": [
    "As we can see, we have 178 samples and 45 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4de0fdb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/2, step 5/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2, step 10/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2, step 15/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2, step 20/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2, step 25/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2, step 30/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2, step 35/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2, step 40/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2, step 45/45, inputs torch.Size([2, 13])\n",
      "epoch 2/2, step 5/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2, step 10/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2, step 15/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2, step 20/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2, step 25/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2, step 30/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2, step 35/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2, step 40/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2, step 45/45, inputs torch.Size([2, 13])\n"
     ]
    }
   ],
   "source": [
    "#now, we will do the actually loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(dataloader):\n",
    "        #forward pass\n",
    "        \n",
    "        #backward pass\n",
    "        \n",
    "        #update weights \n",
    "        \n",
    "        #since this is an example, we will only print information\n",
    "        if (i+1) % 5 == 0:\n",
    "            print(f'epoch {epoch+1}/{num_epochs}, step {i+1}/{n_iterations}, inputs {inputs.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669d7955",
   "metadata": {},
   "source": [
    "So, here we see that we have 2 epochs. In every epoch, we have 45 steps. Every 5th step, we print some information. We see that our tensor is 4x13; our batch size is 4 and 13 features in each batch. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14958401",
   "metadata": {},
   "source": [
    "`pytorch` also has some built in datasets:\n",
    "\n",
    "* `torchvision.datasets.MNIST()`\n",
    "\n",
    "Additionally, you can load the `fashion-mnist`, `cifar` and `coco` datasets. \n",
    "\n",
    "We will see more about this in the next tutorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
